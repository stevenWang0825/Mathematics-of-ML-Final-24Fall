{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiRNN(nn.Module):\n",
    "    def __init__(self, K, MODULUS, hidden_size, num_layers):\n",
    "        super(BiRNN, self).__init__()\n",
    "        # Bidirectional RNN layer\n",
    "        self.rnn = nn.RNN(input_size=1, hidden_size=hidden_size, num_layers = num_layers, bidirectional=True, batch_first=True)\n",
    "        # Fully connected layer for output\n",
    "        self.fc = nn.Linear(2 * hidden_size, MODULUS)  # 2 * hidden_size because it's bidirectional\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(x.shape)\n",
    "        # Forward pass through RNN\n",
    "        out, _ = self.rnn(x)\n",
    "        # Take the output from the last time step\n",
    "        out_last = out[:, -1, :]\n",
    "        # Pass it through the fully connected layer\n",
    "        out_final = self.fc(out_last)\n",
    "        return out_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 4, 1])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_full_multiple_data(MODULUS: int, K :int):\n",
    "    ranges = [range(MODULUS)] * K\n",
    "    combinations = np.array(np.meshgrid(*ranges)).T.reshape(-1, K)\n",
    "    modulo_results = combinations.sum(axis=1) % MODULUS\n",
    "    full_data = np.hstack((combinations, modulo_results.reshape(-1, 1)))\n",
    "    return full_data\n",
    "full_data = generate_full_multiple_data(10, 4)\n",
    "full_data = torch.tensor(full_data, dtype=torch.float32)\n",
    "full_data = full_data.unsqueeze(-1)\n",
    "full_data[:, :-1,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 4, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wangr\\AppData\\Local\\Temp\\ipykernel_24348\\2239002839.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  model(torch.tensor(full_data[:, :-1,:], dtype=torch.float32)), full_data[:, -1,:].shape\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.3736,  0.3143,  0.1453,  ..., -0.2166,  0.1038, -0.1546],\n",
       "         [ 0.3635,  0.3126,  0.1459,  ..., -0.2154,  0.1061, -0.1515],\n",
       "         [ 0.3597,  0.3207,  0.1388,  ..., -0.2136,  0.1108, -0.1553],\n",
       "         ...,\n",
       "         [ 0.4681,  0.3168,  0.0005,  ..., -0.3354,  0.1812, -0.0724],\n",
       "         [ 0.4861,  0.3215, -0.0085,  ..., -0.3292,  0.1854, -0.0824],\n",
       "         [ 0.5007,  0.3258, -0.0150,  ..., -0.3244,  0.1882, -0.0909]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " torch.Size([10000, 1]))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model = BiRNN(K=4, P=10, num_layers=4, hidden_size=5)\n",
    "model(torch.tensor(full_data[:, :-1,:], dtype=torch.float32)), full_data[:, -1,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 4, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wangr\\AppData\\Local\\Temp\\ipykernel_24348\\4182674284.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  criteria(model(torch.tensor(full_data[:, :-1,:], dtype=torch.float32)), full_data[:, -1,:].long().squeeze(-1))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(2.3322, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criteria = nn.CrossEntropyLoss()\n",
    "criteria(model(torch.tensor(full_data[:, :-1,:], dtype=torch.float32)), full_data[:, -1,:].long().squeeze(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_one",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
